# environment
env: Handover-dense
init_grasp_rate: 0.5
goal_ground_rate: 0.5
action_type: continous
grasp_mode: hard
reward_type: sparse
use_her: False
save_render: False
seed: 0

# buffer
buffer_size: 256000

# network
hidden_sizes: [64, 64]

# train
lr: 3.0e-4
epoch: 100
step_per_epoch: 25600000
step_per_collect: 256000 # the number of transitions the collector would collect before the network update
batch_size: 8000
repeat_per_collect: 10 # the number of repeat time for policy learning for each batch

# parallel
device: cuda # cuda
training_num: 64 # 64
test_num: 16 # 10

# PPO
rew_norm: True
vf_coef: 0.25 # [Q] weight for value loss
ent_coef: 0.0 # [Q] weight for entropy loss
gamma: 0.99
gae_lambda: 0.95
bound_action_method: clip
lr_decay: True
max_grad_norm: 0.5
eps_clip: 0.2 
dual_clip: null # [Q]
value_clip: 0 # [Q]
norm_adv: 0 # [Q]
recompute_adv: 1 # [Q]

# dir
logdir: log
resume_path: /Users/reedpan/Downloads/policy.pth

# render
watch_train: False # if show demo directly
render: 0 # render time rate