# environment
env: FetchReach-v1
init_grasp_rate: 0.5
goal_ground_rate: 0.5
action_type: continous
grasp_mode: hard
reward_type: sparse
use_her: False
save_render: False
seed: 0

# buffer
buffer_size: 16384

# network
hidden_sizes: [64, 64]

# train
lr: 2.5e-4
epoch: 10
step_per_epoch: 163840
step_per_collect: 16384 # the number of transitions the collector would collect before the network update
batch_size: 512
repeat_per_collect: 10 # the number of repeat time for policy learning for each batch

# parallel
device: cuda # cuda
training_num: 64
test_num: 10

# PPO
rew_norm: True
vf_coef: 0.5 # [Q] weight for value loss
ent_coef: 0.01 # [Q] weight for entropy loss
gamma: 0.99
gae_lambda: 0.95
bound_action_method: clip
lr_decay: False
max_grad_norm: 0.5
eps_clip: 0.2 
dual_clip: null # [Q]
value_clip: 0 # [Q]
norm_adv: 0 # [Q]
recompute_adv: 1 # [Q]

# dir
logdir: log
resume_path: null

# render
watch_train: False # if show demo directly
render: 0 # render time rate