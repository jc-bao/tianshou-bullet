# environment
env: NaivePickAndPlace-v0
reward_type: sparse
mode: static
error: 0.1
dim: 2
vel: 0.2
use_grasp: False
init_grasp_rate: 0.0
use_her: True
seed: 0

# buffer
buffer_size: 64000000 # 1000000
replay_k: 4

# network
hidden_sizes: [256, 256]

# train
actor_lr: 1.0e-3
critic_lr: 1.0e-3
start_timesteps: 640000 # 10000 # warmup
epoch: 200 # save times
step_per_epoch: 320000 # 5000 # save interval
step_per_collect: 64 # 1 # after collect #, update
update_per_step: 1 # repeat update time
estimation_step: 1 # look ahead time steps
batch_size: 16000 #256

# parallel
device: cuda # cuda
training_num: 64 #1
test_num: 16 # 1

# Algorithm
alpha: 0.2 # entropy regularization coefficient
auto_alpha: False
alpha_lr: 3e-4 # works if open auto alpha
tau: 0.005
gamma: 0.99

# dir
logdir: log
resume_path: null

# render
watch_train: False # if show demo directly
render: 0.02 # render time rate