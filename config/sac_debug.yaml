# environment
env: FetchPickAndPlace-v1
dim: 2
reward_type: sparse
fail_reward: -50
seed: 5

# buffer
buffer_size: 16000000
replay_k: 4

# network
hidden_sizes: [64, 64]

# train
actor_lr: 1.0e-3
critic_lr: 1.0e-3
start_timesteps: 1 # 160000 # warmupÂ·
epoch: 100 # save times
step_per_epoch: 1 # 160000 # save interval (50repeat)
step_per_collect: 1 # 1600 #(64path) after collect #, update
update_per_step: 0.025 # repeat update time
estimation_step: 1 # look ahead time steps
batch_size: 4096

# parallel
device: cuda # cuda
training_num: 1 # 32 # 64
test_num: 1 # 4 # 16

# Algorithm
alpha: 0.2 # entropy regularization coefficient
auto_alpha: False
alpha_lr: 3e-4 # works if open auto alpha
tau: 0.005
gamma: 0.98

# dir
logdir: log
resume_path: null
save_render: False

# render
watch_train: False # if show demo directly
render: 0.00 # render time rate