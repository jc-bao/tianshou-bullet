{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "import yaml\n",
    "\n",
    "import gym, gym_xarm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, ReplayBuffer, VectorReplayBuffer, Batch\n",
    "from tianshou.env import DummyVectorEnv, SubprocVectorEnv\n",
    "from her.offpolicy import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "from tianshou.policy import SACPolicy, BasePolicy\n",
    "\n",
    "import gym_naive\n",
    "from her.her_collector import HERCollector\n",
    "from her.sac_her_policy import SACHERPolicy\n",
    "\n",
    "'''\n",
    "make env\n",
    "'''\n",
    "config = {\n",
    "    'dim': 2,\n",
    "    'reward_type': 'sparse'\n",
    "}\n",
    "def make_env():\n",
    "    return gym.wrappers.FlattenObservation(gym.make('Incremental-v0', config = config))\n",
    "env = gym.make('Incremental-v0', config = config)\n",
    "observation_space = env.observation_space\n",
    "env = gym.wrappers.FlattenObservation(env)\n",
    "obs = (env.reset())\n",
    "state_shape = len(obs)\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "train_envs = SubprocVectorEnv(\n",
    "    [make_env]*2,\n",
    "    norm_obs = False\n",
    ")\n",
    "test_envs = DummyVectorEnv(\n",
    "    [make_env],\n",
    ")\n",
    "\n",
    "'''\n",
    "build and init network\n",
    "'''\n",
    "# actor\n",
    "net_a = Net(state_shape, hidden_sizes=[16], device='cpu')\n",
    "actor = ActorProb(\n",
    "    net_a,\n",
    "    action_shape,\n",
    "    max_action=env.action_space.high[0],\n",
    "    device='cpu',\n",
    "    unbounded=True,\n",
    "    conditioned_sigma=True\n",
    ").to('cpu')\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "# critic\n",
    "net_c1 = Net(\n",
    "    state_shape,\n",
    "    action_shape,\n",
    "    hidden_sizes=[16],\n",
    "    concat=True,\n",
    "    device='cpu'\n",
    ")\n",
    "net_c2 = Net(\n",
    "    state_shape,\n",
    "    action_shape,\n",
    "    hidden_sizes=[16],\n",
    "    concat=True,\n",
    "    device='cpu'\n",
    ")\n",
    "critic1 = Critic(net_c1, device='cpu').to('cpu')\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=0.001)\n",
    "critic2 = Critic(net_c2, device='cpu').to('cpu')\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=0.001)\n",
    "\n",
    "'''\n",
    "set up policy\n",
    "'''\n",
    "policy = SACHERPolicy(\n",
    "    actor,\n",
    "    actor_optim,\n",
    "    critic1,\n",
    "    critic1_optim,\n",
    "    critic2,\n",
    "    critic2_optim,\n",
    "    tau=0.005,\n",
    "    gamma=0.9,\n",
    "    alpha=0.2,\n",
    "    estimation_step=2,\n",
    "    action_space=env.action_space,\n",
    "    reward_normalization = False,\n",
    "    dict_observation_space = observation_space,\n",
    "    reward_fn = env.compute_reward, \n",
    "    future_k = 2,\n",
    ")\n",
    "\n",
    "'''\n",
    "set up collector\n",
    "'''\n",
    "train_buffer = VectorReplayBuffer(64, 2)\n",
    "train_collector = HERCollector(policy, train_envs, train_buffer, exploration_noise=True, observation_space = observation_space, reward_fn = env.compute_reward, k = 2, strategy='offline')\n",
    "# train_collector = Collector(policy, train_envs, train_buffer, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector.env.workers[1].parent_remote.send([\"change\", {'step':3}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[131.,  36.,  26.],\n",
       "        [108.,  13.,   3.]], dtype=float32),\n",
       " array([1., 0.]),\n",
       " array([ True, False]),\n",
       " array([{'is_success': 1.0, 'future_length': -21, 'achieved_goal': None, 'env_id': 0},\n",
       "        {'is_success': 0.0, 'future_length': 2, 'achieved_goal': None, 'env_id': 1}],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collector.env.step([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trainer\n",
    "'''\n",
    "result = offpolicy_trainer(\n",
    "    policy = policy,\n",
    "    train_collector= train_collector,\n",
    "    test_collector= test_collector,\n",
    "    max_epoch= 1,\n",
    "    step_per_epoch= 100,\n",
    "    step_per_collect= 10,\n",
    "    episode_per_test= 1,\n",
    "    batch_size=4,\n",
    "    update_per_step=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gym.spaces.utils import unflatten, flatten\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, buffer in enumerate(train_buffer.buffers):\n",
    "    ####DEBUG\n",
    "    for data in buffer:\n",
    "        obs_dict = unflatten(observation_space, data.obs)\n",
    "        obs_next_dict = unflatten(observation_space, data.obs_next)\n",
    "        data={\n",
    "            'buffer_ID': i,\n",
    "            'obs':obs_dict['observation'][0], \n",
    "            'ag':obs_dict['achieved_goal'][0], \n",
    "            'g':obs_dict['desired_goal'][0], \n",
    "            'obs_n':obs_next_dict['observation'][0], \n",
    "            'ag_n':obs_next_dict['achieved_goal'][0], \n",
    "            'g_n':obs_next_dict['desired_goal'][0], \n",
    "            'done': data.done,\n",
    "            'rew': data.rew,\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "    ####DEBUG\n",
    "df.to_csv('log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "buffer_size = 5\n",
    "done_index = np.array([3,8])\n",
    "current_index = np.array([1,3,4,6,9])\n",
    "final_index = []\n",
    "for idx in current_index:\n",
    "    buffer_idx = int(idx/buffer_size)\n",
    "    current_done_index = done_index[np.logical_and(done_index>idx, done_index<(buffer_idx+1)*buffer_size)]\n",
    "    if len(current_done_index)==0:\n",
    "        final_index.append(idx)\n",
    "    else:\n",
    "        final_index.append(min(current_done_index))\n",
    "print(final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,2,3])\n",
    "a[-1]=2\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5eb6902d1900d2decb3bdf7ac16a0c61011659b39aa72c90b1b2afe1472d5be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
