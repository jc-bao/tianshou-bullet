{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pprint\n",
    "import yaml\n",
    "\n",
    "import gym, gym_xarm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, ReplayBuffer, VectorReplayBuffer, Batch\n",
    "from tianshou.env import DummyVectorEnv, SubprocVectorEnv\n",
    "from offpolicy import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "from tianshou.policy import SACPolicy, BasePolicy\n",
    "\n",
    "import gym_naive\n",
    "from her_collector import HERCollector\n",
    "from sac_her_policy import SACHERPolicy\n",
    "\n",
    "'''\n",
    "make env\n",
    "'''\n",
    "config = {\n",
    "    'dim': 2,\n",
    "    'reward_type': 'sparse'\n",
    "}\n",
    "def make_env():\n",
    "    return gym.wrappers.FlattenObservation(gym.make('Incremental-v0', config = config))\n",
    "env = gym.make('Incremental-v0', config = config)\n",
    "observation_space = env.observation_space\n",
    "env = gym.wrappers.FlattenObservation(env)\n",
    "obs = (env.reset())\n",
    "state_shape = len(obs)\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "train_envs = SubprocVectorEnv(\n",
    "    [make_env]*2,\n",
    "    norm_obs = False\n",
    ")\n",
    "test_envs = DummyVectorEnv(\n",
    "    [make_env],\n",
    ")\n",
    "\n",
    "'''\n",
    "build and init network\n",
    "'''\n",
    "# actor\n",
    "net_a = Net(state_shape, hidden_sizes=[16], device='cpu')\n",
    "actor = ActorProb(\n",
    "    net_a,\n",
    "    action_shape,\n",
    "    max_action=env.action_space.high[0],\n",
    "    device='cpu',\n",
    "    unbounded=True,\n",
    "    conditioned_sigma=True\n",
    ").to('cpu')\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "# critic\n",
    "net_c1 = Net(\n",
    "    state_shape,\n",
    "    action_shape,\n",
    "    hidden_sizes=[16],\n",
    "    concat=True,\n",
    "    device='cpu'\n",
    ")\n",
    "net_c2 = Net(\n",
    "    state_shape,\n",
    "    action_shape,\n",
    "    hidden_sizes=[16],\n",
    "    concat=True,\n",
    "    device='cpu'\n",
    ")\n",
    "critic1 = Critic(net_c1, device='cpu').to('cpu')\n",
    "critic1_optim = torch.optim.Adam(critic1.parameters(), lr=0.001)\n",
    "critic2 = Critic(net_c2, device='cpu').to('cpu')\n",
    "critic2_optim = torch.optim.Adam(critic2.parameters(), lr=0.001)\n",
    "\n",
    "'''\n",
    "set up policy\n",
    "'''\n",
    "policy = SACHERPolicy(\n",
    "    actor,\n",
    "    actor_optim,\n",
    "    critic1,\n",
    "    critic1_optim,\n",
    "    critic2,\n",
    "    critic2_optim,\n",
    "    tau=0.005,\n",
    "    gamma=0.9,\n",
    "    alpha=0.2,\n",
    "    estimation_step=2,\n",
    "    action_space=env.action_space,\n",
    "    reward_normalization = False,\n",
    "    dict_observation_space = observation_space,\n",
    "    reward_fn = env.compute_reward, \n",
    "    future_k = 2,\n",
    ")\n",
    "\n",
    "'''\n",
    "set up collector\n",
    "'''\n",
    "train_buffer = VectorReplayBuffer(128, 2)\n",
    "train_collector = HERCollector(policy, train_envs, train_buffer, exploration_noise=True, observation_space = observation_space, reward_fn = env.compute_reward, k = 2, strategy='online')\n",
    "# train_collector = Collector(policy, train_envs, train_buffer, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector.collect(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  70%|#######   | 70/100 [00:00<00:00, 170.00it/s, env_step=70, len=5, loss/actor=-45.797, loss/critic1=157019.016, loss/critic2=162117.569, n/ep=2, n/st=10, rew=1.00, succeed=1.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(\n",
      "    obs: array([[107.00002, 509.     ,   2.     ],\n",
      "                [497.321  , 107.00002,   0.     ],\n",
      "                [507.80615, 109.     ,   3.     ],\n",
      "                [507.80615, 109.     ,   3.     ]], dtype=float32),\n",
      "    act: array([[-0.97810835],\n",
      "                [-1.        ],\n",
      "                [-1.        ],\n",
      "                [-1.        ]], dtype=float32),\n",
      "    rew: array([[-396.62167  ],\n",
      "                [  -1.0000229],\n",
      "                [   0.       ],\n",
      "                [   0.       ]], dtype=float32),\n",
      "    done: array([False, False,  True,  True]),\n",
      "    obs_next: array([[112.378334, 509.      ,   3.      ],\n",
      "                     [106.      , 107.00002 ,   1.      ],\n",
      "                     [109.      , 109.      ,   4.      ],\n",
      "                     [109.      , 109.      ,   4.      ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([2, 4, 1, 1]),\n",
      "              achieved_goal: array([array([[112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[106.      ],\n",
      "                                    [107.00002 ],\n",
      "                                    [112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[109.]], dtype=float32), array([[109.]], dtype=float32)],\n",
      "                                   dtype=object),\n",
      "              is_success: array([0., 0., 1., 1.]),\n",
      "              env_id: array([0, 0, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[107.00002 , 112.378334,   2.      ],\n",
      "                [112.378334, 509.      ,   3.      ],\n",
      "                [497.321   , 509.      ,   0.      ],\n",
      "                [501.92267 ,  11.      ,   0.      ]], dtype=float32),\n",
      "    act: array([[-0.97810835],\n",
      "                [ 0.9999999 ],\n",
      "                [-1.        ],\n",
      "                [-1.        ]], dtype=float32),\n",
      "    rew: array([[   0.],\n",
      "                [   0.],\n",
      "                [-403.],\n",
      "                [  95.]], dtype=float32),\n",
      "    done: array([False,  True, False, False]),\n",
      "    obs_next: array([[112.378334, 112.378334,   3.      ],\n",
      "                     [509.      , 509.      ,   4.      ],\n",
      "                     [106.      , 509.      ,   1.      ],\n",
      "                     [106.      ,  11.      ,   1.      ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([2, 1, 4, 4]),\n",
      "              achieved_goal: array([array([[112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[509.]], dtype=float32),\n",
      "                                    array([[106.      ],\n",
      "                                    [107.00002 ],\n",
      "                                    [112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[106.      ],\n",
      "                                    [107.06258 ],\n",
      "                                    [108.000854],\n",
      "                                    [157.41391 ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([0., 1., 0., 0.]),\n",
      "              env_id: array([0, 0, 0, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[107.00002 , 509.      ,   2.      ],\n",
      "                [106.      , 108.00017 ,   1.      ],\n",
      "                [107.      ,  13.      ,   2.      ],\n",
      "                [107.06258 , 108.000854,   2.      ]], dtype=float32),\n",
      "    act: array([[-0.97810835],\n",
      "                [-0.9999995 ],\n",
      "                [ 0.9990309 ],\n",
      "                [-0.9999957 ]], dtype=float32),\n",
      "    rew: array([[-396.62167  ],\n",
      "                [  -1.0000763],\n",
      "                [ 494.80615  ],\n",
      "                [   0.       ]], dtype=float32),\n",
      "    done: array([False, False, False, False]),\n",
      "    obs_next: array([[112.378334, 509.      ,   3.      ],\n",
      "                     [107.00009 , 108.00017 ,   2.      ],\n",
      "                     [507.80615 ,  13.      ,   3.      ],\n",
      "                     [108.000854, 108.000854,   3.      ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([2, 3, 2, 2]),\n",
      "              achieved_goal: array([array([[112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[107.00009],\n",
      "                                    [108.00017],\n",
      "                                    [509.     ]], dtype=float32),\n",
      "                                    array([[507.80615],\n",
      "                                    [109.     ]], dtype=float32),\n",
      "                                    array([[108.000854],\n",
      "                                    [157.41391 ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([0., 0., 0., 0.]),\n",
      "              env_id: array([0, 1, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[506.86862,  14.     ,   3.     ],\n",
      "                [506.86862, 109.     ,   3.     ],\n",
      "                [490.42947, 108.     ,   0.     ],\n",
      "                [501.92267, 157.41391,   0.     ]], dtype=float32),\n",
      "    act: array([[-1.],\n",
      "                [-1.],\n",
      "                [-1.],\n",
      "                [-1.]], dtype=float32),\n",
      "    rew: array([[ 95.     ],\n",
      "                [  0.     ],\n",
      "                [ -2.     ],\n",
      "                [-51.41391]], dtype=float32),\n",
      "    done: array([ True,  True, False, False]),\n",
      "    obs_next: array([[109.     ,  14.     ,   4.     ],\n",
      "                     [109.     , 109.     ,   4.     ],\n",
      "                     [106.     , 108.     ,   1.     ],\n",
      "                     [106.     , 157.41391,   1.     ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([1, 1, 4, 4]),\n",
      "              achieved_goal: array([array([[109.]], dtype=float32), array([[109.]], dtype=float32),\n",
      "                                    array([[106.     ],\n",
      "                                    [501.1142 ],\n",
      "                                    [108.     ],\n",
      "                                    [109.00002]], dtype=float32),\n",
      "                                    array([[106.      ],\n",
      "                                    [107.06258 ],\n",
      "                                    [108.000854],\n",
      "                                    [157.41391 ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([1., 1., 0., 0.]),\n",
      "              env_id: array([0, 0, 0, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[112.378334,  14.      ,   3.      ],\n",
      "                [497.321   ,  11.      ,   0.      ],\n",
      "                [501.1142  ,  13.      ,   2.      ],\n",
      "                [507.80615 , 109.      ,   3.      ]], dtype=float32),\n",
      "    act: array([[ 0.9999999],\n",
      "                [-1.       ],\n",
      "                [-1.       ],\n",
      "                [-1.       ]], dtype=float32),\n",
      "    rew: array([[495.],\n",
      "                [ 95.],\n",
      "                [ 95.],\n",
      "                [  0.]], dtype=float32),\n",
      "    done: array([ True, False, False,  True]),\n",
      "    obs_next: array([[509.,  14.,   4.],\n",
      "                     [106.,  11.,   1.],\n",
      "                     [108.,  13.,   3.],\n",
      "                     [109., 109.,   4.]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([1, 4, 2, 1]),\n",
      "              achieved_goal: array([array([[509.]], dtype=float32),\n",
      "                                    array([[106.      ],\n",
      "                                    [107.00002 ],\n",
      "                                    [112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[108.     ],\n",
      "                                    [109.00002]], dtype=float32),\n",
      "                                    array([[109.]], dtype=float32)], dtype=object),\n",
      "              is_success: array([1., 0., 0., 1.]),\n",
      "              env_id: array([0, 0, 0, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[108.      , 508.99152 ,   3.      ],\n",
      "                [490.42947 ,  11.      ,   0.      ],\n",
      "                [106.      , 107.000015,   1.      ],\n",
      "                [107.      , 109.      ,   2.      ]], dtype=float32),\n",
      "    act: array([[ 0.9999576 ],\n",
      "                [-1.        ],\n",
      "                [-0.99999994],\n",
      "                [ 0.9990309 ]], dtype=float32),\n",
      "    rew: array([[  0.     ],\n",
      "                [ 95.     ],\n",
      "                [  0.     ],\n",
      "                [398.80615]], dtype=float32),\n",
      "    done: array([ True, False, False, False]),\n",
      "    obs_next: array([[508.99152 , 508.99152 ,   4.      ],\n",
      "                     [106.      ,  11.      ,   1.      ],\n",
      "                     [107.000015, 107.000015,   2.      ],\n",
      "                     [507.80615 , 109.      ,   3.      ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([1, 4, 3, 2]),\n",
      "              achieved_goal: array([array([[508.99152]], dtype=float32),\n",
      "                                    array([[106.     ],\n",
      "                                    [501.1142 ],\n",
      "                                    [108.     ],\n",
      "                                    [109.00002]], dtype=float32),\n",
      "                                    array([[107.000015],\n",
      "                                    [110.65886 ],\n",
      "                                    [109.00019 ]], dtype=float32),\n",
      "                                    array([[507.80615],\n",
      "                                    [109.     ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([1., 0., 0., 0.]),\n",
      "              env_id: array([0, 0, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[506.86862,  14.     ,   3.     ],\n",
      "                [107.00002, 509.     ,   2.     ],\n",
      "                [107.00009, 108.00017,   2.     ],\n",
      "                [107.00111,  13.     ,   2.     ]], dtype=float32),\n",
      "    act: array([[-1.        ],\n",
      "                [-0.97810835],\n",
      "                [-0.99999917],\n",
      "                [-0.99999994]], dtype=float32),\n",
      "    rew: array([[  95.      ],\n",
      "                [-396.62167 ],\n",
      "                [   0.      ],\n",
      "                [  95.000015]], dtype=float32),\n",
      "    done: array([ True, False, False, False]),\n",
      "    obs_next: array([[109.      ,  14.      ,   4.      ],\n",
      "                     [112.378334, 509.      ,   3.      ],\n",
      "                     [108.00017 , 108.00017 ,   3.      ],\n",
      "                     [108.000015,  13.      ,   3.      ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([1, 2, 2, 2]),\n",
      "              achieved_goal: array([array([[109.]], dtype=float32),\n",
      "                                    array([[112.378334],\n",
      "                                    [509.      ]], dtype=float32),\n",
      "                                    array([[108.00017],\n",
      "                                    [509.     ]], dtype=float32),\n",
      "                                    array([[108.000015],\n",
      "                                    [506.76486 ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([1., 0., 0., 0.]),\n",
      "              env_id: array([0, 0, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[108.01091,  14.     ,   3.     ],\n",
      "                [371.94104,  11.     ,   0.     ],\n",
      "                [106.     ,  12.     ,   1.     ],\n",
      "                [403.16867, 108.00017,   0.     ]], dtype=float32),\n",
      "    act: array([[-0.99995834],\n",
      "                [-1.        ],\n",
      "                [ 1.        ],\n",
      "                [-1.        ]], dtype=float32),\n",
      "    rew: array([[ 95.00833  ],\n",
      "                [ 95.       ],\n",
      "                [495.       ],\n",
      "                [ -2.0001678]], dtype=float32),\n",
      "    done: array([False, False, False, False]),\n",
      "    obs_next: array([[109.00833,  14.     ,   4.     ],\n",
      "                     [106.     ,  11.     ,   1.     ],\n",
      "                     [507.     ,  12.     ,   2.     ],\n",
      "                     [106.     , 108.00017,   1.     ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([1, 4, 3, 4]),\n",
      "              achieved_goal: array([None,\n",
      "                                    array([[106.     ],\n",
      "                                    [107.00002],\n",
      "                                    [503.83276],\n",
      "                                    [109.     ]], dtype=float32),\n",
      "                                    array([[507.     ],\n",
      "                                    [108.     ],\n",
      "                                    [109.70894]], dtype=float32),\n",
      "                                    array([[106.     ],\n",
      "                                    [107.00009],\n",
      "                                    [108.00017],\n",
      "                                    [509.     ]], dtype=float32)], dtype=object),\n",
      "              is_success: array([0., 0., 0., 0.]),\n",
      "              env_id: array([0, 0, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rl/tianshou-bullet/offpolicy.py:130: RuntimeWarning: Mean of empty slice.\n",
      "  logger.write(\"train/env_step\", env_step, {\"train/succeed\": result[\"succeed\"].mean()})  # CHANGE add success record\n",
      "/env/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/rl/tianshou-bullet/offpolicy.py:131: RuntimeWarning: Mean of empty slice.\n",
      "  data['succeed'] = f\"{result['succeed'].mean():.2f}\"\n",
      "Epoch #1: 101it [00:00, 412.14it/s, env_step=100, len=0, loss/actor=-40.614, loss/critic1=147934.795, loss/critic2=152338.101, n/ep=0, n/st=10, rew=0.00, succeed=nan]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(\n",
      "    obs: array([[481.38922 , 109.      ,   0.      ],\n",
      "                [112.      ,  18.      ,   7.      ],\n",
      "                [106.      , 506.76486 ,   1.      ],\n",
      "                [108.000854, 157.41391 ,   3.      ]], dtype=float32),\n",
      "    act: array([[-1.        ],\n",
      "                [-0.99999565],\n",
      "                [-0.99999446],\n",
      "                [-0.75793046]], dtype=float32),\n",
      "    rew: array([[  -3.     ],\n",
      "                [  95.00087],\n",
      "                [-399.76376],\n",
      "                [   0.     ]], dtype=float32),\n",
      "    done: array([False, False, False,  True]),\n",
      "    obs_next: array([[106.     , 109.     ,   1.     ],\n",
      "                     [113.00087,  18.     ,   8.     ],\n",
      "                     [107.00111, 506.76486,   2.     ],\n",
      "                     [157.41391, 157.41391,   4.     ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([ 4, -3,  3,  1]),\n",
      "              achieved_goal: array([array([[106.      ],\n",
      "                                    [107.001945],\n",
      "                                    [506.86862 ],\n",
      "                                    [109.      ]], dtype=float32),\n",
      "                                    None,\n",
      "                                    array([[107.00111 ],\n",
      "                                    [108.000015],\n",
      "                                    [506.76486 ]], dtype=float32),\n",
      "                                    array([[157.41391]], dtype=float32)], dtype=object),\n",
      "              is_success: array([0., 0., 0., 1.]),\n",
      "              env_id: array([0, 1, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Batch(\n",
      "    obs: array([[107.      ,  13.      ,   2.      ],\n",
      "                [108.      , 109.00002 ,   3.      ],\n",
      "                [108.000015,  14.      ,   3.      ],\n",
      "                [108.      , 109.70894 ,   3.      ]], dtype=float32),\n",
      "    act: array([[-0.99994546],\n",
      "                [-0.9999999 ],\n",
      "                [ 0.9888245 ],\n",
      "                [-0.9964553 ]], dtype=float32),\n",
      "    rew: array([[ 95.01091],\n",
      "                [  0.     ],\n",
      "                [492.76486],\n",
      "                [  0.     ]], dtype=float32),\n",
      "    done: array([False,  True,  True,  True]),\n",
      "    obs_next: array([[108.01091,  13.     ,   3.     ],\n",
      "                     [109.00002, 109.00002,   4.     ],\n",
      "                     [506.76486,  14.     ,   4.     ],\n",
      "                     [109.70894, 109.70894,   4.     ]], dtype=float32),\n",
      "    info: Batch(\n",
      "              future_length: array([2, 1, 1, 1]),\n",
      "              achieved_goal: array([None, array([[109.00002]], dtype=float32),\n",
      "                                    array([[506.76486]], dtype=float32),\n",
      "                                    array([[109.70894]], dtype=float32)], dtype=object),\n",
      "              is_success: array([0., 1., 1., 1.]),\n",
      "              env_id: array([0, 0, 1, 1]),\n",
      "          ),\n",
      "    policy: Batch(),\n",
      ")\n",
      "Epoch #1: test_reward: 1.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "trainer\n",
    "'''\n",
    "result = offpolicy_trainer(\n",
    "    policy = policy,\n",
    "    train_collector= train_collector,\n",
    "    test_collector= test_collector,\n",
    "    max_epoch= 1,\n",
    "    step_per_epoch= 100,\n",
    "    step_per_collect= 10,\n",
    "    episode_per_test= 1,\n",
    "    batch_size=4,\n",
    "    update_per_step=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gym.spaces.utils import unflatten, flatten\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, buffer in enumerate(train_buffer.buffers):\n",
    "    ####DEBUG\n",
    "    for data in buffer:\n",
    "        obs_dict = unflatten(observation_space, data.obs)\n",
    "        obs_next_dict = unflatten(observation_space, data.obs_next)\n",
    "        data={\n",
    "            'buffer_ID': i,\n",
    "            'obs':obs_dict['observation'][0], \n",
    "            'ag':obs_dict['achieved_goal'][0], \n",
    "            'g':obs_dict['desired_goal'][0], \n",
    "            'obs_n':obs_next_dict['observation'][0], \n",
    "            'ag_n':obs_next_dict['achieved_goal'][0], \n",
    "            'g_n':obs_next_dict['desired_goal'][0], \n",
    "            'done': data.done,\n",
    "            'rew': data.rew,\n",
    "        }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "    ####DEBUG\n",
    "df.to_csv('log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "buffer_size = 5\n",
    "done_index = np.array([3,8])\n",
    "current_index = np.array([1,3,4,6,9])\n",
    "final_index = []\n",
    "for idx in current_index:\n",
    "    buffer_idx = int(idx/buffer_size)\n",
    "    current_done_index = done_index[np.logical_and(done_index>idx, done_index<(buffer_idx+1)*buffer_size)]\n",
    "    if len(current_done_index)==0:\n",
    "        final_index.append(idx)\n",
    "    else:\n",
    "        final_index.append(min(current_done_index))\n",
    "print(final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5eb6902d1900d2decb3bdf7ac16a0c61011659b39aa72c90b1b2afe1472d5be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
