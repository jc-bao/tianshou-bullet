{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import gym_xarm, yaml, gym, pybulletgym\n",
    "import datetime, os, pprint\n",
    "import numpy as np\n",
    "import gym_naive\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Independent, Normal\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import onpolicy_trainer\n",
    "from tianshou.data import Collector, ReplayBuffer, VectorReplayBuffer\n",
    "\n",
    "# Note: do not remove __main__ as it will use multi-process\n",
    "'''\n",
    "load param\n",
    "'''\n",
    "with open(\"config/ppo_bullet.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "'''\n",
    "make env\n",
    "'''\n",
    "env = gym.make(config['env'], config = config)\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape\n",
    "max_action = env.action_space.high[0]\n",
    "env.close()\n",
    "test_envs = DummyVectorEnv(\n",
    "    [lambda: gym.make(config['env'], config=config)],\n",
    "    norm_obs = True,\n",
    "    update_obs_rms = False\n",
    ")\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "test_envs.seed(config['seed'])\n",
    "\n",
    "'''\n",
    "build and init network\n",
    "'''\n",
    "# if not (torch.cuda.is_available()):\n",
    "config['device'] = 'cpu'\n",
    "net_a = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes=config['hidden_sizes'],\n",
    "    activation=nn.Tanh,\n",
    "    device=config['device']\n",
    ")\n",
    "actor = ActorProb(\n",
    "    net_a,\n",
    "    action_shape,\n",
    "    max_action = max_action,\n",
    "    unbounded=True,\n",
    "    device=config['device']\n",
    ").to(config['device'])\n",
    "net_c = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes=config['hidden_sizes'],\n",
    "    activation=nn.Tanh,\n",
    "    device=config['device']\n",
    ")\n",
    "critic = Critic(net_c, device=config['device']).to(config['device'])\n",
    "# init parameters\n",
    "torch.nn.init.constant_(actor.sigma_param, -0.5)\n",
    "for m in list(actor.modules()) + list(critic.modules()):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        # orthogonal initialization\n",
    "        torch.nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "for m in actor.mu.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        m.weight.data.copy_(0.01 * m.weight.data)\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    list(actor.parameters()) + list(critic.parameters()), lr=config['lr']\n",
    ")\n",
    "lr_scheduler = None\n",
    "if config['lr_decay']:\n",
    "    # decay learning rate to 0 linearly\n",
    "    max_update_num = np.ceil(\n",
    "        config['step_per_epoch'] / config['step_per_collect']\n",
    "    ) * config['epoch']\n",
    "    # [Q]\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optim, lr_lambda=lambda epoch: 1 - epoch / max_update_num\n",
    "    )\n",
    "\n",
    "'''\n",
    "set up policy\n",
    "'''\n",
    "# distribution function to get action\n",
    "def dist(*logits):\n",
    "    return Independent(Normal(*logits), 1)\n",
    "policy = PPOPolicy(\n",
    "    actor,\n",
    "    critic,\n",
    "    optim,\n",
    "    dist,\n",
    "    discount_factor=config['gamma'],\n",
    "    gae_lambda=config['gae_lambda'],\n",
    "    max_grad_norm=config['max_grad_norm'],\n",
    "    vf_coef=config['vf_coef'],\n",
    "    ent_coef=config['ent_coef'],\n",
    "    reward_normalization=config['rew_norm'],\n",
    "    action_scaling=True,\n",
    "    action_bound_method=config['bound_action_method'],\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    action_space=env.action_space,\n",
    "    eps_clip=config['eps_clip'],\n",
    "    value_clip=config['value_clip'],\n",
    "    dual_clip=config['dual_clip'],\n",
    "    advantage_normalization=config['norm_adv'],\n",
    "    recompute_advantage=config['recompute_adv']\n",
    ")\n",
    "# load previous policy\n",
    "policy.load_state_dict(torch.load('/rl/tianshou-bullet/log/NaivePickAndPlace-v0/ppo/buf100k_step1g_up100k_batch1k_rewnormF_fixEpLen_init05_err01/[good]o2g/policy.pth', map_location=config['device']))\n",
    "\n",
    "'''\n",
    "set up collector\n",
    "'''\n",
    "test_collector = Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture saved in  /rl/tianshou-bullet/pic/pic0.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic1.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic2.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic3.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic4.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic5.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic6.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic7.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic8.png\n",
      "picture saved in  /rl/tianshou-bullet/pic/pic9.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "render\n",
    "'''\n",
    "policy.eval()\n",
    "\n",
    "test_collector.reset()\n",
    "result = test_collector.collect(n_episode=10, render=0.000001)\n",
    "# success_rate = ((result[\"info\"]['is_success'] != 50).astype(np.float32)).mean()\n",
    "# print(f'Final reward: {result[\"rews\"].mean()}, length: {result[\"lens\"].mean()}, success rate = {success_rate}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
